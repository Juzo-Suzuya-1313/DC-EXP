# -*- coding: utf-8 -*-
"""Exp4_SMOTE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c9rjqNn8OeGLGuS92enS6icxTphosuTt
"""

# ✅ Step 1: Import libraries
import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# ✅ Step 2: Generate synthetic imbalanced dataset
# We'll use make_classification to simulate an imbalanced dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,
                           n_clusters_per_class=1, weights=[0.9, 0.1], flip_y=0, random_state=42)

# Show class distribution before SMOTE
print("Class distribution before SMOTE:", pd.Series(y).value_counts())

# ✅ Step 3: Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ✅ Step 4: Apply SMOTE to balance the class distribution
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Show class distribution after SMOTE
print("Class distribution after SMOTE:", pd.Series(y_train_res).value_counts())

# ✅ Step 5: Train a classifier (Random Forest) on the resampled data
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_res, y_train_res)

# ✅ Step 6: Evaluate the model
y_pred = clf.predict(X_test)
print("\nClassification Report on Test Data:")
print(classification_report(y_test, y_pred))
print("Accuracy Score:", accuracy_score(y_test, y_pred))